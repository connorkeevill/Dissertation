%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Connor Keevill at 2022-11-07 12:31:25 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@proceedings{Whelan:2015aa,
	annote = {Robotics: Science and Systems (RSS); 2330-765X; http://hdl.handle.net/10044/1/23438; Robotics: Science and Systems XI},
	author = {Whelan, T and Leutenegger, S and Salas-Moreno, R and Glocker, B and Davison, A},
	date-added = {2022-11-07 12:24:25 +0000},
	date-modified = {2022-11-07 12:24:25 +0000},
	id = {TN{\_}cdi{\_}imperial{\_}dspace{\_}oai{\_}spiral{\_}imperial{\_}ac{\_}uk{\_}10044{\_}1{\_}23438},
	isbn = {2330-765X},
	publisher = {Robotics: Science and Systems},
	title = {ElasticFusion: dense SLAM without a pose graph},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAxRWxhc3RpY0Z1c2lvbiBkZW5zZSBzbGFtIHdpdGhvdXQgYSBwb3NlIGdyYXBoLnBkZk8RAnQAAAAAAnQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAN90XFVCRAAB/////x9FbGFzdGljRnVzaW9uIGRlbnMjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////346pSgAAAAAAAAAAAAEAAgAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAnC86VXNlcnM6Y29ubm9yOkxpYnJhcnk6Q2xvdWRTdG9yYWdlOk9uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGg6NHRoIFllYXI6Q00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdDpQYXBlcnM6RWxhc3RpY0Z1c2lvbiBkZW5zZSBzbGFtIHdpdGhvdXQgYSBwb3NlIGdyYXBoLnBkZgAOAGQAMQBFAGwAYQBzAHQAaQBjAEYAdQBzAGkAbwBuACAAZABlAG4AcwBlACAAcwBsAGEAbQAgAHcAaQB0AGgAbwB1AHQAIABhACAAcABvAHMAZQAgAGcAcgBhAHAAaAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAmlVzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvUGFwZXJzL0VsYXN0aWNGdXNpb24gZGVuc2Ugc2xhbSB3aXRob3V0IGEgcG9zZSBncmFwaC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAWAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAALQ}}

@article{doi:10.1177/0278364916669237,
	abstract = { We present a novel approach to real-time dense visual simultaneous localisation and mapping. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using an RGB-D camera in an incremental online fashion, without pose graph optimization or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimizations as often as possible to stay close to the mode of the map distribution, while utilizing global loop closure to recover from arbitrary drift and maintain global consistency. In the spirit of improving map quality as well as tracking accuracy and robustness, we furthermore explore a novel approach to real-time discrete light source detection. This technique is capable of detecting numerous light sources in indoor environments in real-time as a user handheld camera explores the scene. Absolutely no prior information about the scene or number of light sources is required. By making a small set of simple assumptions about the appearance properties of the scene our method can incrementally estimate both the quantity and location of multiple light sources in the environment in an online fashion. Our results demonstrate that our technique functions well in many different environments and lighting configurations. We show that this enables (a) more realistic augmented reality rendering; (b) a richer understanding of the scene beyond pure geometry and; (c) more accurate and robust photometric tracking. },
	author = {Thomas Whelan and Renato F Salas-Moreno and Ben Glocker and Andrew J Davison and Stefan Leutenegger},
	date-added = {2022-11-07 12:20:10 +0000},
	date-modified = {2022-11-07 12:31:17 +0000},
	doi = {10.1177/0278364916669237},
	eprint = {https://doi.org/10.1177/0278364916669237},
	journal = {The International Journal of Robotics Research},
	number = {14},
	pages = {1697-1716},
	title = {ElasticFusion: Real-time dense SLAM and light source estimation},
	url = {https://doi.org/10.1177/0278364916669237},
	volume = {35},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBCRWxhc3RpY0Z1c2lvbiByZWFsLXRpbWUgZGVuc2Ugc2xhbSBhbmQgbGlnaHQgc291cmNlIGVzdGltYXRpb24ucGRmTxECugAAAAACugACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA33RcVUJEAAH/////H0VsYXN0aWNGdXNpb24gcmVhbCNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////fjqRmAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgCtLzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OlBhcGVyczpFbGFzdGljRnVzaW9uIHJlYWwtdGltZSBkZW5zZSBzbGFtIGFuZCBsaWdodCBzb3VyY2UgZXN0aW1hdGlvbi5wZGYAAA4AhgBCAEUAbABhAHMAdABpAGMARgB1AHMAaQBvAG4AIAByAGUAYQBsAC0AdABpAG0AZQAgAGQAZQBuAHMAZQAgAHMAbABhAG0AIABhAG4AZAAgAGwAaQBnAGgAdAAgAHMAbwB1AHIAYwBlACAAZQBzAHQAaQBtAGEAdABpAG8AbgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAq1VzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvUGFwZXJzL0VsYXN0aWNGdXNpb24gcmVhbC10aW1lIGRlbnNlIHNsYW0gYW5kIGxpZ2h0IHNvdXJjZSBlc3RpbWF0aW9uLnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQAaQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAMn},
	bdsk-url-1 = {https://doi.org/10.1177/0278364916669237}}

@article{Li:2022aa,
	abstract = {High-quality 3D reconstruction is an important topic in computer graphics and computer vision with many applications, such as robotics and augmented reality. The advent of consumer RGB-D cameras has made a profound advance in indoor scene reconstruction. For the past few years, researchers have spent significant effort to develop algorithms to capture 3D models with RGB-D cameras. As depth images produced by consumer RGB-D cameras are noisy and incomplete when surfaces are shiny, bright, transparent, or far from the camera, obtaining high-quality 3D scene models is still a challenge for existing systems. We here review high-quality 3D indoor scene reconstruction methods using consumer RGB-D cameras. In this paper, we make comparisons and analyses from the following aspects: (i) depth processing methods in 3D reconstruction are reviewed in terms of enhancement and completion, (ii) ICP-based, feature-based, and hybrid methods of camera pose estimation methods are reviewed, and (iii) surface reconstruction methods are reviewed in terms of surface fusion, optimization, and completion. The performance of state-of-the-art methods is also compared and analyzed. This survey will be useful for researchers who want to follow best practices in designing new high-quality 3D reconstruction methods.},
	author = {Li, Jianwei and Gao, Wei and Wu, Yihong and Liu, Yangdong and Shen, Yanfei},
	date = {2022/09/01},
	date-added = {2022-11-01 14:33:08 +0000},
	date-modified = {2022-11-04 15:46:20 +0000},
	doi = {10.1007/s41095-021-0250-8},
	id = {Li2022},
	isbn = {2096-0662},
	journal = {Computational Visual Media},
	number = {3},
	pages = {369--393},
	read = {1},
	title = {High-quality indoor scene 3D reconstruction with RGB-D cameras: A brief review},
	url = {https://doi.org/10.1007/s41095-021-0250-8},
	volume = {8},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAvSGlnaCBxdWFsaXR5IGluZG9vciAzRCBzY2VuZSByZWNvbnN0cnVjdGlvbi5wZGZPEQJsAAAAAAJsAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfdFxVQkQAAf////8fSGlnaCBxdWFsaXR5IGluZG9vI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+BrCUAAAAAAAAAAAABAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAJovOlVzZXJzOmNvbm5vcjpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoOjR0aCBZZWFyOkNNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3Q6UGFwZXJzOkhpZ2ggcXVhbGl0eSBpbmRvb3IgM0Qgc2NlbmUgcmVjb25zdHJ1Y3Rpb24ucGRmAA4AYAAvAEgAaQBnAGgAIABxAHUAYQBsAGkAdAB5ACAAaQBuAGQAbwBvAHIAIAAzAEQAIABzAGMAZQBuAGUAIAByAGUAYwBvAG4AcwB0AHIAdQBjAHQAaQBvAG4ALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAJhVc2Vycy9jb25ub3IvTGlicmFyeS9DbG91ZFN0b3JhZ2UvT25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aC80dGggWWVhci9DTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0L1BhcGVycy9IaWdoIHF1YWxpdHkgaW5kb29yIDNEIHNjZW5lIHJlY29uc3RydWN0aW9uLnBkZgATAAEvAAAVAAIADf//AAAACAANABoAJABWAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAsY=},
	bdsk-url-1 = {https://doi.org/10.1007/s41095-021-0250-8}}
