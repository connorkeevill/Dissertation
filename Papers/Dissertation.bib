%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Connor Keevill at 2022-11-21 21:41:30 +0000 


%% Saved with string encoding Unicode (UTF-8) 



@article{Gokturk2004ATD,
	author = {S. Burak Gokturk and Hakan Yalcin and Cyrus S. Bamji},
	date-added = {2022-11-21 21:31:59 +0000},
	date-modified = {2022-11-21 21:32:30 +0000},
	journal = {2004 Conference on Computer Vision and Pattern Recognition Workshop},
	keywords = {Time of Flight depth},
	pages = {35-35},
	title = {A Time-Of-Flight Depth Sensor - System Description, Issues and Solutions},
	year = {2004},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBLQV9UaW1lLU9mLUZsaWdodF9EZXB0aF9TZW5zb3JfLV9TeXN0ZW1fRGVzY3JpcHRpb25fSXNzdWVzX2FuZF9Tb2x1dGlvbnMucGRmTxEDDAAAAAADDAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H0FfVGltZS1PZi1GbGlnaHRfRCNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////foZ6OAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgDOLzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpBX1RpbWUtT2YtRmxpZ2h0X0RlcHRoX1NlbnNvcl8tX1N5c3RlbV9EZXNjcmlwdGlvbl9Jc3N1ZXNfYW5kX1NvbHV0aW9ucy5wZGYADgCYAEsAQQBfAFQAaQBtAGUALQBPAGYALQBGAGwAaQBnAGgAdABfAEQAZQBwAHQAaABfAFMAZQBuAHMAbwByAF8ALQBfAFMAeQBzAHQAZQBtAF8ARABlAHMAYwByAGkAcAB0AGkAbwBuAF8ASQBzAHMAdQBlAHMAXwBhAG4AZABfAFMAbwBsAHUAdABpAG8AbgBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgDMVXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvQV9UaW1lLU9mLUZsaWdodF9EZXB0aF9TZW5zb3JfLV9TeXN0ZW1fRGVzY3JpcHRpb25fSXNzdWVzX2FuZF9Tb2x1dGlvbnMucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAHIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADgg==}}

@inproceedings{5539794,
	abstract = {We present a method which enables rapid and dense reconstruction of scenes browsed by a single live camera. We take point-based real-time structure from motion (SFM) as our starting point, generating accurate 3D camera pose estimates and a sparse point cloud. Our main novel contribution is to use an approximate but smooth base mesh generated from the SFM to predict the view at a bundle of poses around automatically selected reference frames spanning the scene, and then warp the base mesh into highly accurate depth maps based on view-predictive optical flow and a constrained scene flow update. The quality of the resulting depth maps means that a convincing global scene model can be obtained simply by placing them side by side and removing overlapping regions. We show that a cluttered indoor environment can be reconstructed from a live hand-held camera in a few seconds, with all processing performed by current desktop hardware. Real-time monocular dense reconstruction opens up many application areas, and we demonstrate both real-time novel view synthesis and advanced augmented reality where augmentations interact physically with the 3D scene and are correctly clipped by occlusions.},
	author = {Newcombe, Richard A. and Davison, Andrew J.},
	booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	date-added = {2022-11-11 17:02:48 +0000},
	date-modified = {2022-11-11 17:03:45 +0000},
	doi = {10.1109/CVPR.2010.5539794},
	issn = {1063-6919},
	keywords = {Struggle with Dynamics, From KinectFusion},
	month = {June},
	pages = {1498-1505},
	title = {Live dense reconstruction with a single moving camera},
	year = {2010},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5TGl2ZV9kZW5zZV9yZWNvbnN0cnVjdGlvbl93aXRoX2Ffc2luZ2xlX21vdmluZ19jYW1lcmEucGRmTxECxAAAAAACxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H0xpdmVfZGVuc2VfcmVjb25zdCNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////flDCYAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgC8LzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpMaXZlX2RlbnNlX3JlY29uc3RydWN0aW9uX3dpdGhfYV9zaW5nbGVfbW92aW5nX2NhbWVyYS5wZGYADgB0ADkATABpAHYAZQBfAGQAZQBuAHMAZQBfAHIAZQBjAG8AbgBzAHQAcgB1AGMAdABpAG8AbgBfAHcAaQB0AGgAXwBhAF8AcwBpAG4AZwBsAGUAXwBtAG8AdgBpAG4AZwBfAGMAYQBtAGUAcgBhAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgC6VXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvTGl2ZV9kZW5zZV9yZWNvbnN0cnVjdGlvbl93aXRoX2Ffc2luZ2xlX21vdmluZ19jYW1lcmEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAGAAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADKA==},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR.2010.5539794}}

@inproceedings{6126513,
	abstract = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.},
	author = {Newcombe, Richard A. and Lovegrove, Steven J. and Davison, Andrew J.},
	booktitle = {2011 International Conference on Computer Vision},
	date-added = {2022-11-11 17:00:47 +0000},
	date-modified = {2022-11-11 17:04:08 +0000},
	doi = {10.1109/ICCV.2011.6126513},
	issn = {2380-7504},
	keywords = {Struggle with Dynamics, From KinectFusion},
	month = {Nov},
	pages = {2320-2327},
	title = {DTAM: Dense tracking and mapping in real-time},
	year = {2011},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAwRFRBTV9EZW5zZV90cmFja2luZ19hbmRfbWFwcGluZ19pbl9yZWFsLXRpbWUucGRmTxECogAAAAACogACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H0RUQU1fRGVuc2VfdHJhY2tpbiNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////flDAnAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgCzLzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpEVEFNX0RlbnNlX3RyYWNraW5nX2FuZF9tYXBwaW5nX2luX3JlYWwtdGltZS5wZGYAAA4AYgAwAEQAVABBAE0AXwBEAGUAbgBzAGUAXwB0AHIAYQBjAGsAaQBuAGcAXwBhAG4AZABfAG0AYQBwAHAAaQBuAGcAXwBpAG4AXwByAGUAYQBsAC0AdABpAG0AZQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAsVVzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvRG9jdW1lbnRzICYgU3VibWlzc2lvbnMvUGFwZXJzL0RUQU1fRGVuc2VfdHJhY2tpbmdfYW5kX21hcHBpbmdfaW5fcmVhbC10aW1lLnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQAVwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAL9},
	bdsk-url-1 = {https://doi.org/10.1109/ICCV.2011.6126513}}

@inproceedings{4408984,
	abstract = {We present a viewpoint-based approach for the quick fusion of multiple stereo depth maps. Our method selects depth estimates for each pixel that minimize violations of visibility constraints and thus remove errors and inconsistencies from the depth maps to produce a consistent surface. We advocate a two-stage process in which the first stage generates potentially noisy, overlapping depth maps from a set of calibrated images and the second stage fuses these depth maps to obtain an integrated surface with higher accuracy, suppressed noise, and reduced redundancy. We show that by dividing the processing into two stages we are able to achieve a very high throughput because we are able to use a computationally cheap stereo algorithm and because this architecture is amenable to hardware-accelerated (GPU) implementations. A rigorous formulation based on the notion of stability of a depth estimate is presented first. It aims to determine the validity of a depth estimate by rendering multiple depth maps into the reference view as well as rendering the reference depth map into the other views in order to detect occlusions and free- space violations. We also present an approximate alternative formulation that selects and validates only one hypothesis based on confidence. Both formulations enable us to perform video-based reconstruction at up to 25 frames per second. We show results on the multi-view stereo evaluation benchmark datasets and several outdoors video sequences. Extensive quantitative analysis is performed using an accurately surveyed model of a real building as ground truth.},
	author = {Merrell, Paul and Akbarzadeh, Amir and Wang, Liang and Mordohai, Philippos and Frahm, Jan-Michael and Yang, Ruigang and Nister, David and Pollefeys, Marc},
	booktitle = {2007 IEEE 11th International Conference on Computer Vision},
	date-added = {2022-11-11 16:58:32 +0000},
	date-modified = {2022-11-11 17:04:18 +0000},
	doi = {10.1109/ICCV.2007.4408984},
	issn = {2380-7504},
	keywords = {Struggle with Dynamics, From KinectFusion},
	month = {Oct},
	pages = {1-8},
	title = {Real-Time Visibility-Based Fusion of Depth Maps},
	year = {2007},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAzUmVhbC1UaW1lX1Zpc2liaWxpdHktQmFzZWRfRnVzaW9uX29mX0RlcHRoX01hcHMucGRmTxECrAAAAAACrAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H1JlYWwtVGltZV9WaXNpYmlsaSNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////flC+VAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgC2LzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpSZWFsLVRpbWVfVmlzaWJpbGl0eS1CYXNlZF9GdXNpb25fb2ZfRGVwdGhfTWFwcy5wZGYADgBoADMAUgBlAGEAbAAtAFQAaQBtAGUAXwBWAGkAcwBpAGIAaQBsAGkAdAB5AC0AQgBhAHMAZQBkAF8ARgB1AHMAaQBvAG4AXwBvAGYAXwBEAGUAcAB0AGgAXwBNAGEAcABzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgC0VXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvUmVhbC1UaW1lX1Zpc2liaWxpdHktQmFzZWRfRnVzaW9uX29mX0RlcHRoX01hcHMucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAFoAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADCg==},
	bdsk-url-1 = {https://doi.org/10.1109/ICCV.2007.4408984}}

@inproceedings{6162880,
	abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
	annote = {This is the paper that is cited by the review paper},
	author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
	bdsk-color = {4},
	booktitle = {2011 10th IEEE International Symposium on Mixed and Augmented Reality},
	date-added = {2022-11-11 16:42:40 +0000},
	date-modified = {2022-11-11 16:45:07 +0000},
	doi = {10.1109/ISMAR.2011.6092378},
	month = {Oct},
	pages = {127-136},
	title = {KinectFusion: Real-time dense surface mapping and tracking},
	year = {2011},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA+S2luZWN0RnVzaW9uIC0gcmVhbHRpbWUgZGVuc2Ugc3VyZmFjZSBtYXBwaW5nIGFuZCB0cmFja2luZy5wZGZPEQLaAAAAAALaAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgVTFQkQAAf////8fS2luZWN0RnVzaW9uIC0gcmVhI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+UK6UAAAAAAAAAAAABAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAMEvOlVzZXJzOmNvbm5vcjpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoOjR0aCBZZWFyOkNNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3Q6RG9jdW1lbnRzICYgU3VibWlzc2lvbnM6UGFwZXJzOktpbmVjdEZ1c2lvbiAtIHJlYWx0aW1lIGRlbnNlIHN1cmZhY2UgbWFwcGluZyBhbmQgdHJhY2tpbmcucGRmAAAOAH4APgBLAGkAbgBlAGMAdABGAHUAcwBpAG8AbgAgAC0AIAByAGUAYQBsAHQAaQBtAGUAIABkAGUAbgBzAGUAIABzAHUAcgBmAGEAYwBlACAAbQBhAHAAcABpAG4AZwAgAGEAbgBkACAAdAByAGEAYwBrAGkAbgBnAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgC/VXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvS2luZWN0RnVzaW9uIC0gcmVhbHRpbWUgZGVuc2Ugc3VyZmFjZSBtYXBwaW5nIGFuZCB0cmFja2luZy5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAGUAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADQw==},
	bdsk-url-1 = {https://doi.org/10.1109/ISMAR.2011.6092378}}

@inproceedings{izadi2011kinectfusion,
	abstract = {KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. We show uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch.},
	annote = {This is the paper published by Microsoft recommended by Mac
},
	author = {Izadi, Shahram and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin and Davison, Andrew and Fitzgibbon, Andrew},
	bdsk-color = {4},
	booktitle = {UIST '11 Proceedings of the 24th annual ACM symposium on User interface software and technology},
	date-added = {2022-11-11 15:41:30 +0000},
	date-modified = {2022-11-15 14:37:41 +0000},
	month = {October},
	pages = {559-568},
	publisher = {ACM},
	title = {KinectFusion: Real-time 3D Reconstruction and Interaction Using a Moving Depth Camera},
	url = {https://www.microsoft.com/en-us/research/publication/kinectfusion-real-time-3d-reconstruction-and-interaction-using-a-moving-depth-camera/},
	year = {2011},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBXS2luZWN0RnVzaW9uIHJlYWwtdGltZSAzRCByZWNvbnN0cnVjdGlvbiBhbmQgaW50ZXJhdGlvbiB1c2luZyBhIG1vdmluZyBkZXB0aCBjYW1lcmEucGRmTxEDPAAAAAADPAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H0tpbmVjdEZ1c2lvbiByZWFsLSNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////flB1HAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgDaLzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpLaW5lY3RGdXNpb24gcmVhbC10aW1lIDNEIHJlY29uc3RydWN0aW9uIGFuZCBpbnRlcmF0aW9uIHVzaW5nIGEgbW92aW5nIGRlcHRoIGNhbWVyYS5wZGYADgCwAFcASwBpAG4AZQBjAHQARgB1AHMAaQBvAG4AIAByAGUAYQBsAC0AdABpAG0AZQAgADMARAAgAHIAZQBjAG8AbgBzAHQAcgB1AGMAdABpAG8AbgAgAGEAbgBkACAAaQBuAHQAZQByAGEAdABpAG8AbgAgAHUAcwBpAG4AZwAgAGEAIABtAG8AdgBpAG4AZwAgAGQAZQBwAHQAaAAgAGMAYQBtAGUAcgBhAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgDYVXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvS2luZWN0RnVzaW9uIHJlYWwtdGltZSAzRCByZWNvbnN0cnVjdGlvbiBhbmQgaW50ZXJhdGlvbiB1c2luZyBhIG1vdmluZyBkZXB0aCBjYW1lcmEucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAH4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADvg==},
	bdsk-url-1 = {https://www.microsoft.com/en-us/research/publication/kinectfusion-real-time-3d-reconstruction-and-interaction-using-a-moving-depth-camera/}}

@article{Zhang:2018aa,
	abstract = {Real-time indoor scene reconstruction aims to recover the 3D geometry of an indoor scene in real time with a sensor scanning the scene. Previous works of this topic consider pure static scenes, but in this paper, we focus on more challenging cases that the scene contains dynamic objects, for example, moving people and floating curtains, which are quite common in reality and thus are eagerly required to be handled. We develop an end-to-end system using a depth sensor to scan a scene on the fly. By proposing a Sigmoid-based Iterative Closest Point (S-ICP) method, we decouple the camera motion and the scene motion from the input sequence and segment the scene into static and dynamic parts accordingly. The static part is used to estimate the camera rigid motion, while for the dynamic part, graph node-based motion representation and model-to-depth fitting are applied to reconstruct the scene motions. With the camera and scene motions reconstructed, we further propose a novel mixed voxel allocation scheme to handle static and dynamic scene parts with different mechanisms, which helps to gradually fuse a large scene with both static and dynamic objects. Experiments show that our technique successfully fuses the geometry of both the static and dynamic objects in a scene in real time, which extends the usage of the current techniques for indoor scene reconstruction.},
	author = {H. Zhang and F. Xu},
	date-added = {2022-11-11 13:37:05 +0000},
	date-modified = {2022-11-11 15:33:03 +0000},
	doi = {10.1109/TVCG.2017.2786233},
	isbn = {1941-0506},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	journal1 = {IEEE Transactions on Visualization and Computer Graphics},
	journal2 = {IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Dynamic Reconstruction},
	number = {12},
	pages = {3137--3146},
	title = {MixedFusion: Real-Time Reconstruction of an Indoor Scene with Dynamic Objects},
	vo = {24},
	volume = {24},
	year = {2018},
	year1 = {1 Dec. 2018},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBQTWl4ZWRGdXNpb25fUmVhbC1UaW1lX1JlY29uc3RydWN0aW9uX29mX2FuX0luZG9vcl9TY2VuZV93aXRoX0R5bmFtaWNfT2JqZWN0cy5wZGZPEQMiAAAAAAMiAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgVTFQkQAAf////8fTWl4ZWRGdXNpb25fUmVhbC1UI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+UAIkAAAAAAAAAAAABAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACANMvOlVzZXJzOmNvbm5vcjpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoOjR0aCBZZWFyOkNNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3Q6RG9jdW1lbnRzICYgU3VibWlzc2lvbnM6UGFwZXJzOk1peGVkRnVzaW9uX1JlYWwtVGltZV9SZWNvbnN0cnVjdGlvbl9vZl9hbl9JbmRvb3JfU2NlbmVfd2l0aF9EeW5hbWljX09iamVjdHMucGRmAAAOAKIAUABNAGkAeABlAGQARgB1AHMAaQBvAG4AXwBSAGUAYQBsAC0AVABpAG0AZQBfAFIAZQBjAG8AbgBzAHQAcgB1AGMAdABpAG8AbgBfAG8AZgBfAGEAbgBfAEkAbgBkAG8AbwByAF8AUwBjAGUAbgBlAF8AdwBpAHQAaABfAEQAeQBuAGEAbQBpAGMAXwBPAGIAagBlAGMAdABzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgDRVXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvTWl4ZWRGdXNpb25fUmVhbC1UaW1lX1JlY29uc3RydWN0aW9uX29mX2FuX0luZG9vcl9TY2VuZV93aXRoX0R5bmFtaWNfT2JqZWN0cy5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAHcAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADnQ==},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2786233}}

@inproceedings{7299179,
	abstract = {The popularity of low-cost RGB-D scanners is increasing on a daily basis. Nevertheless, existing scanners often cannot capture subtle details in the environment. We present a novel method to enhance the depth map by fusing the intensity and depth information to create more detailed range profiles. The lighting model we use can handle natural scene illumination. It is integrated in a shape from shading like technique to improve the visual fidelity of the reconstructed object. Unlike previous efforts in this domain, the detailed geometry is calculated directly, without the need to explicitly find and integrate surface normals. In addition, the proposed method operates four orders of magnitude faster than the state of the art. Qualitative and quantitative visual and statistical evidence support the improvement in the depth obtained by the suggested method.},
	author = {Or - El, Roy and Rosman, Guy and Wetzler, Aaron and Kimmel, Ron and Bruckstein, Alfred M.},
	booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2022-11-11 11:45:38 +0000},
	date-modified = {2022-11-11 15:32:29 +0000},
	doi = {10.1109/CVPR.2015.7299179},
	issn = {1063-6919},
	keywords = {SFS},
	month = {June},
	pages = {5407-5416},
	read = {1},
	title = {RGBD-fusion: Real-time high precision depth recovery},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA3UkdCRC1mdXNpb25fUmVhbC10aW1lX2hpZ2hfcHJlY2lzaW9uX2RlcHRoX3JlY292ZXJ5LnBkZk8RArwAAAAAArwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAN+BVMVCRAAB/////x9SR0JELWZ1c2lvbl9SZWFsLXQjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////35PmSwAAAAAAAAAAAAEAAgAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAui86VXNlcnM6Y29ubm9yOkxpYnJhcnk6Q2xvdWRTdG9yYWdlOk9uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGg6NHRoIFllYXI6Q00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdDpEb2N1bWVudHMgJiBTdWJtaXNzaW9uczpQYXBlcnM6UkdCRC1mdXNpb25fUmVhbC10aW1lX2hpZ2hfcHJlY2lzaW9uX2RlcHRoX3JlY292ZXJ5LnBkZgAOAHAANwBSAEcAQgBEAC0AZgB1AHMAaQBvAG4AXwBSAGUAYQBsAC0AdABpAG0AZQBfAGgAaQBnAGgAXwBwAHIAZQBjAGkAcwBpAG8AbgBfAGQAZQBwAHQAaABfAHIAZQBjAG8AdgBlAHIAeQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAuFVzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvRG9jdW1lbnRzICYgU3VibWlzc2lvbnMvUGFwZXJzL1JHQkQtZnVzaW9uX1JlYWwtdGltZV9oaWdoX3ByZWNpc2lvbl9kZXB0aF9yZWNvdmVyeS5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAXgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAMe},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR.2015.7299179}}

@article{Yang:2017aa,
	abstract = {Geometry completion is an important operation for generating a complete model. In this paper, we present a novel geometry completion algorithm for point cloud models, which is capable of filling holes on either smooth models or surfaces with sharp features. Our method is built on the physical diffusion pattern. We first decompose each pass hole-boundary contraction into two steps, namely normal propagation and position sampling. Then the normal dissimilarity constraint is incorporated into these two steps to fill holes with sharp features. Our algorithm implements these two steps alternately and terminates until generating no new hole boundary. Experimental results demonstrate its feasibility and validity of recovering the potential geometry shapes.},
	author = {Yang, Long and Yan, Qingan and Xiao, Chunxia},
	date = {2017/03/01},
	date-added = {2022-11-09 10:49:23 +0000},
	date-modified = {2022-11-09 10:49:23 +0000},
	doi = {10.1007/s00371-016-1208-1},
	id = {Yang2017},
	isbn = {1432-2315},
	journal = {The Visual Computer},
	number = {3},
	pages = {385--398},
	title = {Shape-controllable geometry completion for point cloud models},
	url = {https://doi.org/10.1007/s00371-016-1208-1},
	volume = {33},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBBU2hhcGUtY29udHJvbGxhYmxlIGdlb21ldHJ5IGNvbXBsZXRpb24gZm9yIHBvaW50IGNsb3VkIG1vZGVscy5wZGZPEQLkAAAAAALkAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgVTFQkQAAf////8fU2hhcGUtY29udHJvbGxhYmxlI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+RNeMAAAAAAAAAAAABAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAMQvOlVzZXJzOmNvbm5vcjpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoOjR0aCBZZWFyOkNNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3Q6RG9jdW1lbnRzICYgU3VibWlzc2lvbnM6UGFwZXJzOlNoYXBlLWNvbnRyb2xsYWJsZSBnZW9tZXRyeSBjb21wbGV0aW9uIGZvciBwb2ludCBjbG91ZCBtb2RlbHMucGRmAA4AhABBAFMAaABhAHAAZQAtAGMAbwBuAHQAcgBvAGwAbABhAGIAbABlACAAZwBlAG8AbQBlAHQAcgB5ACAAYwBvAG0AcABsAGUAdABpAG8AbgAgAGYAbwByACAAcABvAGkAbgB0ACAAYwBsAG8AdQBkACAAbQBvAGQAZQBsAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAMJVc2Vycy9jb25ub3IvTGlicmFyeS9DbG91ZFN0b3JhZ2UvT25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aC80dGggWWVhci9DTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0L0RvY3VtZW50cyAmIFN1Ym1pc3Npb25zL1BhcGVycy9TaGFwZS1jb250cm9sbGFibGUgZ2VvbWV0cnkgY29tcGxldGlvbiBmb3IgcG9pbnQgY2xvdWQgbW9kZWxzLnBkZgATAAEvAAAVAAIADf//AAAACAANABoAJABoAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA1A=},
	bdsk-url-1 = {https://doi.org/10.1007/s00371-016-1208-1}}

@proceedings{Fu:2020aa,
	author = {Y. Fu and Q. Yan and J. Liao and C. Xiao},
	booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2022-11-08 17:30:35 +0000},
	date-modified = {2022-11-11 13:35:37 +0000},
	doi = {10.1109/CVPR42600.2020.00599},
	isbn = {2575-7075},
	journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	journal1 = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	keywords = {Texture Mapping},
	pages = {5949--5958},
	title = {Joint Texture and Geometry Optimization for RGB-D Reconstruction},
	year = {2020},
	year1 = {13-19 June 2020},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBESm9pbnQgVGV4dHVyZSBhbmQgR2VvbWV0cnkgT3B0aW1pemF0aW9uIGZvciBSR0ItRCBSZWNvbnN0cnVjdGlvbi5wZGZPEQLyAAAAAALyAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgVTFQkQAAf////8fSm9pbnQgVGV4dHVyZSBhbmQgI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+QQYwAAAAAAAAAAAABAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAMcvOlVzZXJzOmNvbm5vcjpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoOjR0aCBZZWFyOkNNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3Q6RG9jdW1lbnRzICYgU3VibWlzc2lvbnM6UGFwZXJzOkpvaW50IFRleHR1cmUgYW5kIEdlb21ldHJ5IE9wdGltaXphdGlvbiBmb3IgUkdCLUQgUmVjb25zdHJ1Y3Rpb24ucGRmAAAOAIoARABKAG8AaQBuAHQAIABUAGUAeAB0AHUAcgBlACAAYQBuAGQAIABHAGUAbwBtAGUAdAByAHkAIABPAHAAdABpAG0AaQB6AGEAdABpAG8AbgAgAGYAbwByACAAUgBHAEIALQBEACAAUgBlAGMAbwBuAHMAdAByAHUAYwB0AGkAbwBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgDFVXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvSm9pbnQgVGV4dHVyZSBhbmQgR2VvbWV0cnkgT3B0aW1pemF0aW9uIGZvciBSR0ItRCBSZWNvbnN0cnVjdGlvbi5wZGYAABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAGsAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADYQ==},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR42600.2020.00599}}

@proceedings{Fu:2018aa,
	author = {Y. Fu and Q. Yan and L. Yang and J. Liao and C. Xiao},
	booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	date-added = {2022-11-08 17:30:35 +0000},
	date-modified = {2022-11-11 13:35:53 +0000},
	doi = {10.1109/CVPR.2018.00488},
	isbn = {2575-7075},
	journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	journal1 = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	keywords = {Texture Mapping},
	pages = {4645--4653},
	read = {1},
	title = {Texture Mapping for 3D Reconstruction with RGB-D Sensor},
	year = {2018},
	year1 = {18-23 June 2018},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA7VGV4dHVyZSBNYXBwaW5nIGZvciAzRCBSZWNvbnN0cnVjdGlvbiB3aXRoIFJHQi1EIFNlbnNvci5wZGZPEQLMAAAAAALMAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgVTFQkQAAf////8fVGV4dHVyZSBNYXBwaW5nIGZvI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+QQYwAAAAAAAAAAAABAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAL4vOlVzZXJzOmNvbm5vcjpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoOjR0aCBZZWFyOkNNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3Q6RG9jdW1lbnRzICYgU3VibWlzc2lvbnM6UGFwZXJzOlRleHR1cmUgTWFwcGluZyBmb3IgM0QgUmVjb25zdHJ1Y3Rpb24gd2l0aCBSR0ItRCBTZW5zb3IucGRmAA4AeAA7AFQAZQB4AHQAdQByAGUAIABNAGEAcABwAGkAbgBnACAAZgBvAHIAIAAzAEQAIABSAGUAYwBvAG4AcwB0AHIAdQBjAHQAaQBvAG4AIAB3AGkAdABoACAAUgBHAEIALQBEACAAUwBlAG4AcwBvAHIALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASALxVc2Vycy9jb25ub3IvTGlicmFyeS9DbG91ZFN0b3JhZ2UvT25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aC80dGggWWVhci9DTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0L0RvY3VtZW50cyAmIFN1Ym1pc3Npb25zL1BhcGVycy9UZXh0dXJlIE1hcHBpbmcgZm9yIDNEIFJlY29uc3RydWN0aW9uIHdpdGggUkdCLUQgU2Vuc29yLnBkZgATAAEvAAAVAAIADf//AAAACAANABoAJABiAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAzI=},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR.2018.00488}}

@article{DBLP:journals/corr/abs-1810-00729,
	author = {Thomas Sch{\"{o}}ps and Torsten Sattler and Marc Pollefeys},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1810-00729.bib},
	date-added = {2022-11-08 17:30:14 +0000},
	date-modified = {2022-11-08 17:30:14 +0000},
	eprint = {1810.00729},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
	title = {SurfelMeshing: Online Surfel-Based Mesh Reconstruction},
	url = {http://arxiv.org/abs/1810.00729},
	volume = {abs/1810.00729},
	year = {2018},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5U3VyZmVsTWVzaGluZyBPbmxpbmUgU3VyZmVsLUJhc2VkIE1lc2ggUmVjb25zdHJ1Y3Rpb24ucGRmTxECxAAAAAACxAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H1N1cmZlbE1lc2hpbmcgT25saSNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////fkAZfAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgC8LzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpTdXJmZWxNZXNoaW5nIE9ubGluZSBTdXJmZWwtQmFzZWQgTWVzaCBSZWNvbnN0cnVjdGlvbi5wZGYADgB0ADkAUwB1AHIAZgBlAGwATQBlAHMAaABpAG4AZwAgAE8AbgBsAGkAbgBlACAAUwB1AHIAZgBlAGwALQBCAGEAcwBlAGQAIABNAGUAcwBoACAAUgBlAGMAbwBuAHMAdAByAHUAYwB0AGkAbwBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgC6VXNlcnMvY29ubm9yL0xpYnJhcnkvQ2xvdWRTdG9yYWdlL09uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGgvNHRoIFllYXIvQ00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdC9Eb2N1bWVudHMgJiBTdWJtaXNzaW9ucy9QYXBlcnMvU3VyZmVsTWVzaGluZyBPbmxpbmUgU3VyZmVsLUJhc2VkIE1lc2ggUmVjb25zdHJ1Y3Rpb24ucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAGAAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADKA==},
	bdsk-url-1 = {http://arxiv.org/abs/1810.00729}}

@proceedings{Keller:2013aa,
	author = {M. Keller and D. Lefloch and M. Lambers and S. Izadi and T. Weyrich and A. Kolb},
	booktitle = {2013 International Conference on 3D Vision - 3DV 2013},
	date-added = {2022-11-07 19:55:35 +0000},
	date-modified = {2022-11-07 19:55:35 +0000},
	doi = {10.1109/3DV.2013.9},
	isbn = {1550-6185},
	journal = {2013 International Conference on 3D Vision - 3DV 2013},
	journal1 = {2013 International Conference on 3D Vision - 3DV 2013},
	pages = {1--8},
	title = {Real-Time 3D Reconstruction in Dynamic Scenes Using Point-Based Fusion},
	year = {2013},
	year1 = {29 June-1 July 2013},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBJUmVhbCB0aW1lIDNEIHJlY29uc3J1Y3Rpb24gaW4gZHluYW1pYyBzY2VuZXMgdXNpbmcgcG9pbnQtYmFzZWQgZnVzaW9uLnBkZk8RAwQAAAAAAwQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAN+BVMVCRAAB/////x9SZWFsIHRpbWUgM0QgcmVjb24jRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////35A55QAAAAAAAAAAAAEAAgAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAzC86VXNlcnM6Y29ubm9yOkxpYnJhcnk6Q2xvdWRTdG9yYWdlOk9uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGg6NHRoIFllYXI6Q00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdDpEb2N1bWVudHMgJiBTdWJtaXNzaW9uczpQYXBlcnM6UmVhbCB0aW1lIDNEIHJlY29uc3J1Y3Rpb24gaW4gZHluYW1pYyBzY2VuZXMgdXNpbmcgcG9pbnQtYmFzZWQgZnVzaW9uLnBkZgAOAJQASQBSAGUAYQBsACAAdABpAG0AZQAgADMARAAgAHIAZQBjAG8AbgBzAHIAdQBjAHQAaQBvAG4AIABpAG4AIABkAHkAbgBhAG0AaQBjACAAcwBjAGUAbgBlAHMAIAB1AHMAaQBuAGcAIABwAG8AaQBuAHQALQBiAGEAcwBlAGQAIABmAHUAcwBpAG8AbgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAylVzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvRG9jdW1lbnRzICYgU3VibWlzc2lvbnMvUGFwZXJzL1JlYWwgdGltZSAzRCByZWNvbnNydWN0aW9uIGluIGR5bmFtaWMgc2NlbmVzIHVzaW5nIHBvaW50LWJhc2VkIGZ1c2lvbi5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAcAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAN4},
	bdsk-url-1 = {https://doi.org/10.1109/3DV.2013.9}}

@proceedings{Whelan:2015aa,
	annote = {Robotics: Science and Systems (RSS); 2330-765X; http://hdl.handle.net/10044/1/23438; Robotics: Science and Systems XI},
	author = {Whelan, T and Leutenegger, S and Salas-Moreno, R and Glocker, B and Davison, A},
	date-added = {2022-11-07 12:24:25 +0000},
	date-modified = {2022-11-07 12:24:25 +0000},
	id = {TN{\_}cdi{\_}imperial{\_}dspace{\_}oai{\_}spiral{\_}imperial{\_}ac{\_}uk{\_}10044{\_}1{\_}23438},
	isbn = {2330-765X},
	publisher = {Robotics: Science and Systems},
	title = {ElasticFusion: dense SLAM without a pose graph},
	year = {2015},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAxRWxhc3RpY0Z1c2lvbiBkZW5zZSBzbGFtIHdpdGhvdXQgYSBwb3NlIGdyYXBoLnBkZk8RAqQAAAAAAqQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAN+BVMVCRAAB/////x9FbGFzdGljRnVzaW9uIGRlbnMjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////35A55AAAAAAAAAAAAAEAAgAACiBjdQAAAAAAAAAAAAAAAAAGUGFwZXJzAAIAtC86VXNlcnM6Y29ubm9yOkxpYnJhcnk6Q2xvdWRTdG9yYWdlOk9uZURyaXZlLVVuaXZlcnNpdHlvZkJhdGg6NHRoIFllYXI6Q00zMDA4MiAtIEluZGl2aWR1YWwgUHJvamVjdDpEb2N1bWVudHMgJiBTdWJtaXNzaW9uczpQYXBlcnM6RWxhc3RpY0Z1c2lvbiBkZW5zZSBzbGFtIHdpdGhvdXQgYSBwb3NlIGdyYXBoLnBkZgAOAGQAMQBFAGwAYQBzAHQAaQBjAEYAdQBzAGkAbwBuACAAZABlAG4AcwBlACAAcwBsAGEAbQAgAHcAaQB0AGgAbwB1AHQAIABhACAAcABvAHMAZQAgAGcAcgBhAHAAaAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAslVzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvRG9jdW1lbnRzICYgU3VibWlzc2lvbnMvUGFwZXJzL0VsYXN0aWNGdXNpb24gZGVuc2Ugc2xhbSB3aXRob3V0IGEgcG9zZSBncmFwaC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAWAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAMA}}

@article{doi:10.1177/0278364916669237,
	abstract = { We present a novel approach to real-time dense visual simultaneous localisation and mapping. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using an RGB-D camera in an incremental online fashion, without pose graph optimization or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimizations as often as possible to stay close to the mode of the map distribution, while utilizing global loop closure to recover from arbitrary drift and maintain global consistency. In the spirit of improving map quality as well as tracking accuracy and robustness, we furthermore explore a novel approach to real-time discrete light source detection. This technique is capable of detecting numerous light sources in indoor environments in real-time as a user handheld camera explores the scene. Absolutely no prior information about the scene or number of light sources is required. By making a small set of simple assumptions about the appearance properties of the scene our method can incrementally estimate both the quantity and location of multiple light sources in the environment in an online fashion. Our results demonstrate that our technique functions well in many different environments and lighting configurations. We show that this enables (a) more realistic augmented reality rendering; (b) a richer understanding of the scene beyond pure geometry and; (c) more accurate and robust photometric tracking. },
	author = {Thomas Whelan and Renato F Salas-Moreno and Ben Glocker and Andrew J Davison and Stefan Leutenegger},
	date-added = {2022-11-07 12:20:10 +0000},
	date-modified = {2022-11-11 13:36:14 +0000},
	doi = {10.1177/0278364916669237},
	eprint = {https://doi.org/10.1177/0278364916669237},
	journal = {The International Journal of Robotics Research},
	keywords = {Surfel, SLAM},
	number = {14},
	pages = {1697-1716},
	read = {1},
	title = {ElasticFusion: Real-time dense SLAM and light source estimation},
	url = {https://doi.org/10.1177/0278364916669237},
	volume = {35},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBCRWxhc3RpY0Z1c2lvbiByZWFsLXRpbWUgZGVuc2Ugc2xhbSBhbmQgbGlnaHQgc291cmNlIGVzdGltYXRpb24ucGRmTxEC6gAAAAAC6gACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H0VsYXN0aWNGdXNpb24gcmVhbCNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////fkDnkAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgDFLzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpFbGFzdGljRnVzaW9uIHJlYWwtdGltZSBkZW5zZSBzbGFtIGFuZCBsaWdodCBzb3VyY2UgZXN0aW1hdGlvbi5wZGYAAA4AhgBCAEUAbABhAHMAdABpAGMARgB1AHMAaQBvAG4AIAByAGUAYQBsAC0AdABpAG0AZQAgAGQAZQBuAHMAZQAgAHMAbABhAG0AIABhAG4AZAAgAGwAaQBnAGgAdAAgAHMAbwB1AHIAYwBlACAAZQBzAHQAaQBtAGEAdABpAG8AbgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAw1VzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvRG9jdW1lbnRzICYgU3VibWlzc2lvbnMvUGFwZXJzL0VsYXN0aWNGdXNpb24gcmVhbC10aW1lIGRlbnNlIHNsYW0gYW5kIGxpZ2h0IHNvdXJjZSBlc3RpbWF0aW9uLnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQAaQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAANX},
	bdsk-url-1 = {https://doi.org/10.1177/0278364916669237}}

@article{Li:2022aa,
	abstract = {High-quality 3D reconstruction is an important topic in computer graphics and computer vision with many applications, such as robotics and augmented reality. The advent of consumer RGB-D cameras has made a profound advance in indoor scene reconstruction. For the past few years, researchers have spent significant effort to develop algorithms to capture 3D models with RGB-D cameras. As depth images produced by consumer RGB-D cameras are noisy and incomplete when surfaces are shiny, bright, transparent, or far from the camera, obtaining high-quality 3D scene models is still a challenge for existing systems. We here review high-quality 3D indoor scene reconstruction methods using consumer RGB-D cameras. In this paper, we make comparisons and analyses from the following aspects: (i) depth processing methods in 3D reconstruction are reviewed in terms of enhancement and completion, (ii) ICP-based, feature-based, and hybrid methods of camera pose estimation methods are reviewed, and (iii) surface reconstruction methods are reviewed in terms of surface fusion, optimization, and completion. The performance of state-of-the-art methods is also compared and analyzed. This survey will be useful for researchers who want to follow best practices in designing new high-quality 3D reconstruction methods.},
	annote = {what can be seen in here
},
	author = {Li, Jianwei and Gao, Wei and Wu, Yihong and Liu, Yangdong and Shen, Yanfei},
	date = {2022/09/01},
	date-added = {2022-11-01 14:33:08 +0000},
	date-modified = {2022-11-11 13:39:07 +0000},
	doi = {10.1007/s41095-021-0250-8},
	id = {Li2022},
	isbn = {2096-0662},
	journal = {Computational Visual Media},
	keywords = {Review},
	number = {3},
	pages = {369--393},
	read = {1},
	title = {High-quality indoor scene 3D reconstruction with RGB-D cameras: A brief review},
	url = {https://doi.org/10.1007/s41095-021-0250-8},
	volume = {8},
	year = {2022},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAvSGlnaCBxdWFsaXR5IGluZG9vciAzRCBzY2VuZSByZWNvbnN0cnVjdGlvbi5wZGZPEQKcAAAAAAKcAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgVTFQkQAAf////8fSGlnaCBxdWFsaXR5IGluZG9vI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+QOeQAAAAAAAAAAAABAAIAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACALIvOlVzZXJzOmNvbm5vcjpMaWJyYXJ5OkNsb3VkU3RvcmFnZTpPbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoOjR0aCBZZWFyOkNNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3Q6RG9jdW1lbnRzICYgU3VibWlzc2lvbnM6UGFwZXJzOkhpZ2ggcXVhbGl0eSBpbmRvb3IgM0Qgc2NlbmUgcmVjb25zdHJ1Y3Rpb24ucGRmAA4AYAAvAEgAaQBnAGgAIABxAHUAYQBsAGkAdAB5ACAAaQBuAGQAbwBvAHIAIAAzAEQAIABzAGMAZQBuAGUAIAByAGUAYwBvAG4AcwB0AHIAdQBjAHQAaQBvAG4ALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASALBVc2Vycy9jb25ub3IvTGlicmFyeS9DbG91ZFN0b3JhZ2UvT25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aC80dGggWWVhci9DTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0L0RvY3VtZW50cyAmIFN1Ym1pc3Npb25zL1BhcGVycy9IaWdoIHF1YWxpdHkgaW5kb29yIDNEIHNjZW5lIHJlY29uc3RydWN0aW9uLnBkZgATAAEvAAAVAAIADf//AAAACAANABoAJABWAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAvY=},
	bdsk-url-1 = {https://doi.org/10.1007/s41095-021-0250-8}}

@inproceedings{arkitscenes,
	author = {Gilad Baruch and Zhuoyuan Chen and Afshin Dehghan and Tal Dimry and Yuri Feigin and Peter Fu and Thomas Gebauer and Brandon Joffe and Daniel Kurz and Arik Schwartz and Elad Shulman},
	date-modified = {2022-11-21 21:41:22 +0000},
	keywords = {Good Citations},
	title = {ARKitScenes - A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data},
	url = {https://arxiv.org/pdf/2111.08897.pdf},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/pdf/2111.08897.pdf},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBmQVJLaXRTY2VuZXMgQSBEaXZlcnNlIFJlYWwtV29ybGQgRGF0YXNldCBGb3IgM0QgSW5kb29yIFNjZW5lIFVuZGVyc3RhbmRpbmcgVXNpbmcgTW9iaWxlIFJHQi1EIERhdGEucGRmTxEDegAAAAADegACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA34FUxUJEAAH/////H0FSS2l0U2NlbmVzIEEgRGl2ZSNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////foaDSAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAZQYXBlcnMAAgDpLzpVc2Vyczpjb25ub3I6TGlicmFyeTpDbG91ZFN0b3JhZ2U6T25lRHJpdmUtVW5pdmVyc2l0eW9mQmF0aDo0dGggWWVhcjpDTTMwMDgyIC0gSW5kaXZpZHVhbCBQcm9qZWN0OkRvY3VtZW50cyAmIFN1Ym1pc3Npb25zOlBhcGVyczpBUktpdFNjZW5lcyBBIERpdmVyc2UgUmVhbC1Xb3JsZCBEYXRhc2V0IEZvciAzRCBJbmRvb3IgU2NlbmUgVW5kZXJzdGFuZGluZyBVc2luZyBNb2JpbGUgUkdCLUQgRGF0YS5wZGYAAA4AzgBmAEEAUgBLAGkAdABTAGMAZQBuAGUAcwAgAEEAIABEAGkAdgBlAHIAcwBlACAAUgBlAGEAbAAtAFcAbwByAGwAZAAgAEQAYQB0AGEAcwBlAHQAIABGAG8AcgAgADMARAAgAEkAbgBkAG8AbwByACAAUwBjAGUAbgBlACAAVQBuAGQAZQByAHMAdABhAG4AZABpAG4AZwAgAFUAcwBpAG4AZwAgAE0AbwBiAGkAbABlACAAUgBHAEIALQBEACAARABhAHQAYQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIA51VzZXJzL2Nvbm5vci9MaWJyYXJ5L0Nsb3VkU3RvcmFnZS9PbmVEcml2ZS1Vbml2ZXJzaXR5b2ZCYXRoLzR0aCBZZWFyL0NNMzAwODIgLSBJbmRpdmlkdWFsIFByb2plY3QvRG9jdW1lbnRzICYgU3VibWlzc2lvbnMvUGFwZXJzL0FSS2l0U2NlbmVzIEEgRGl2ZXJzZSBSZWFsLVdvcmxkIERhdGFzZXQgRm9yIDNEIEluZG9vciBTY2VuZSBVbmRlcnN0YW5kaW5nIFVzaW5nIE1vYmlsZSBSR0ItRCBEYXRhLnBkZgAAEwABLwAAFQACAA3//wAAAAgADQAaACQAjQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAQL}}
